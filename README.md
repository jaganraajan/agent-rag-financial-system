# Agent RAG Financial System

A comprehensive financial analysis system that combines **Enhanced RAG (Retrieval-Augmented Generation)** capabilities with **LangGraph query decomposition** and a web scraper for SEC EDGAR 10-K filings. The system processes SEC filings, creates searchable vector embeddings, and provides intelligent answers to complex financial questions about Google, Microsoft, and NVIDIA using their recent 10-K filings.

> **Note:** The file `script_output.txt` contains sample outputs for various query types, including comparative, multi-company, and temporal analyses. Refer to this file for example results and expected answer formats.

## üöÄ New Features with LangGraph Integration

### üß† Query Decomposition
- **Automatic Question Breaking**: Complex questions are automatically decomposed into simpler sub-queries
- **Multi-step Retrieval**: Executes multiple targeted searches for comprehensive answers
- **Comparative Analysis**: Handles "which company" and comparison questions intelligently

## üîÅ Compound Query Planner (Iterative, LLM-in-the-loop)

The Compound Query Planner handles multi-step questions where later sub-queries depend on facts discovered in earlier steps. It uses Azure OpenAI to plan each step and the Enhanced RAG pipeline to execute sub-queries against SEC filings.

- Why: Decompose chained questions like ‚ÄúWhich company had the highest revenue in 2024? What are the main AI risks of that company?‚Äù where the second part needs the result of the first.
- How: Loop of Plan ‚Üí Execute ‚Üí Update ‚Üí Repeat, with Azure OpenAI proposing the next sub-query.

### Quick start

1) Configure Azure OpenAI (environment variables):
- AZURE_OPENAI_ENDPOINT
- AZURE_OPENAI_API_KEY
- AZURE_OPENAI_API_VERSION (default: 2024-02-01)
- AZURE_OPENAI_MODEL (e.g., gpt-4o-mini)

2) Minimal usage:
```python
from src.agents.compound_query_planner import CompoundQueryPlanner

planner = CompoundQueryPlanner(max_steps=4, top_k=5)
result = planner.run("Which company had the highest revenue in 2024? What are the main AI risks of that company?")

print(result["final_answer"])
# Inspect intermediate steps:
for s in result["steps"]:
    print(s["subquery"], "->", s["answer"][:200])
```

Notes:
- Under the hood, each sub-query is executed via EnhancedRAGPipeline.query, and the planner uses prior answers to craft the next sub-query.

### ASCII flow

```
User Query
   ‚îÇ
   ‚ñº
[Planner LLM]
   - Propose next_subquery based on original query + previous answers
   ‚îÇ
   ‚ñº
[Execute Sub-query]
   - EnhancedRAGPipeline.query(next_subquery)
   - Retrieve chunks + synthesize short answer
   ‚îÇ
   ‚ñº
[Update Context]
   - Append step answer + sources
   - Surface discovered entities (e.g., MSFT)
   ‚îÇ
   ‚ñº
[Stop?]
   ‚îú‚îÄ Yes ‚Üí [Final Synthesis LLM] ‚Üí Final Answer
   ‚îî‚îÄ No  ‚Üí Loop back to [Planner LLM] with updated context
```

Example chain:
- Step 1: ‚ÄúWhich company had the highest revenue in 2024?‚Äù ‚Üí Answer mentions MSFT.
- Step 2: ‚ÄúWhat are the main AI risks of MSFT?‚Äù ‚Üí Uses MSFT discovered in Step 1.

### üìä Enhanced Query Types
- **Comparative Queries**: "Which company had the highest operating margin in 2023?"
- **Multi-Company Analysis**: "Compare Microsoft and Google revenue"
- **Temporal Analysis**: "What was NVIDIA's growth from 2022 to 2023?"

### üìã Structured JSON Output
```json
{
  "query": "Which company had the highest operating margin in 2023?",
  "answer": "MSFT had the highest operating margin at 42.1% in 2023, followed by NVDA at 32.1%",
  "reasoning": "Retrieved operating margins for 3 companies from their 2023 10-K filings...",
  "sub_queries": ["MSFT operating margin 2023", "GOOGL operating margin 2023", "NVDA operating margin 2023"],
  "sources": [
    {
      "company": "MSFT",
      "year": "2023", 
      "excerpt": "Operating margin was 42.1%...",
      "page": 10
    }
  ],
  "confidence": 0.9
}
```

## Features

### üîç SEC EDGAR Web Scraper
- Downloads 10-K filings for GOOGL, MSFT, and NVDA (2022-2024)
- Handles rate limiting and SEC compliance
- Robust error handling and retry logic
- Demo mode for offline testing

### ü§ñ Enhanced RAG Pipeline
- **Query Decomposition**: LangGraph-powered analysis of complex questions
- **Multi-step Retrieval**: Executes multiple targeted sub-queries
- **Synthesis Engine**: Combines results with reasoning and confidence scoring
- **Text Extraction**: Parses HTML filings and extracts clean text
- **Semantic Chunking**: Splits documents into 50-1000 token chunks with semantic boundaries
- **Embeddings**: Uses Azure OpenAI embeddings (with offline mock fallback)
- **Vector Storage**: ChromaDB for efficient similarity search
- **Dual Interface**: Enhanced JSON output and traditional query modes
- **Metadata**: Extracts company, year, and filing information

### üí° Key Capabilities
- **Intelligent Question Analysis**: Automatically detects comparative, temporal, and simple queries
- **Multi-Company Comparisons**: Handles cross-company financial analysis
- **Structured Reasoning**: Provides explanations for how answers were derived
- **Source Attribution**: Tracks and cites specific document sections
- **Confidence Scoring**: Assigns confidence levels to generated answers
- **Fallback Support**: Gracefully handles errors with basic RAG fallback

- **SEC EDGAR Web Scraper**: Automated downloading of 10-K filings from the SEC EDGAR database
- **Multi-Company Support**: Covers Google (GOOGL), Microsoft (MSFT), and NVIDIA (NVDA)
- **Multi-Year Coverage**: Downloads filings for 2022, 2023, and 2024
- **Rate-Limited API Access**: Respectful scraping with proper delays and user-agent headers
- **Comprehensive Error Handling**: Robust error handling for network issues and missing data
- **Structured Output**: Organized file naming and directory structure

## SEC EDGAR Web Scraper

### Requirements Met

The web scraper satisfies the following data scope requirements:

| Aspect | Requirement | Implementation |
|--------|-------------|----------------|
| **Companies** | Google (GOOGL), Microsoft (MSFT), NVIDIA (NVDA) | ‚úÖ Implemented with CIK codes |
| **Documents** | Annual 10-K filings only | ‚úÖ Filters for 10-K form type |
| **Years** | 2022, 2023, 2024 | ‚úÖ Configurable year range |
| **Total Files** | 9 documents (3 companies √ó 3 years) | ‚úÖ Downloads all combinations |
| **Source** | SEC EDGAR database | ‚úÖ Uses official SEC API |

### Company CIK Codes

- **GOOGL (Alphabet Inc.)**: 1652044
- **MSFT (Microsoft Corporation)**: 789019
- **NVDA (NVIDIA Corporation)**: 1045810


## Installation & Setup

### 1. Clone the repository
```bash
git clone https://github.com/jaganraajan/agent-rag-financial-system.git
cd agent-rag-financial-system
```

### 2. Install dependencies
```bash
pip install -r requirements.txt
```

### 3. (Optional) Set up Azure OpenAI credentials
```bash
export AZURE_OPENAI_ENDPOINT="your-endpoint"
export AZURE_OPENAI_API_KEY="your-api-key"
```

---

## RAG Pipeline Usage

### Process Documents
```bash
python main.py rag --process --input-dir demo_filings
```

### Query the System
```bash
# Enhanced comparative queries
python main.py rag --query "Which company had the highest operating margin in 2023?"

# Multi-company comparisons
python main.py rag --query "Compare Microsoft and Google revenue"

# Traditional single queries
python main.py rag --query "What are NVIDIA's main risk factors?"

# Interactive mode
python main.py rag
```

### More Query Examples
```bash
# Operating margin comparison
python main.py rag --query "Which company had the highest operating margin in 2023?"

# Revenue analysis
python main.py rag --query "Compare MSFT and GOOGL revenue in 2022"

# Growth analysis
python main.py rag --query "What was NVIDIA's growth rate from 2022 to 2023?"

# Force basic mode (without LangGraph features)
python main.py rag --basic --query "Your question"
```

---

## SEC EDGAR Scraper Usage

### Download All Filings
```bash
python main.py scrape
```

### Custom Downloads
```bash
# Download specific companies
python main.py scrape --companies GOOGL MSFT --years 2023 2024

# Custom output directory
python main.py scrape --output-dir my_filings

# Custom user agent
python main.py scrape --user-agent "My Analysis Tool 1.0"
```

---

## ChromaDB Inspector UI (Chunk/Embedding Browser)

### 1. Start the UI server
```bash
python chromadb_ui.py
```

### 2. Open your browser
Navigate to: [http://localhost:8080](http://localhost:8080)

#### Features
- Dashboard: Collection stats, token distribution, content breakdown
- Browse Chunks: Filter and inspect chunk metadata/content
- Semantic Search: Query chunks with similarity scoring

#### API Endpoints
- `GET /api/stats` - Collection statistics
- `GET /api/chunks` - List all chunks (with optional filtering)
- `GET /api/search` - Semantic search

---

## Typical Workflow

1. **Download filings** (optional, for real data):
  ```bash
  python main.py scrape
  ```
2. **Process filings into vector DB:**
  ```bash
  python main.py rag --process --input-dir filings
  ```
3. **Query the system:**
  ```bash
  python main.py rag --query "Your financial question"
  ```
4. **Inspect chunks/embeddings:**
  ```bash
  python chromadb_ui.py
  # Then open http://localhost:8080
  ```

---

## Troubleshooting

- If you see errors about missing filings, check your scrape parameters and output directory.
- For UI issues, ensure chromadb_ui.py is running and your browser is pointed to the correct port.
- For Azure OpenAI, ensure your credentials are set as environment variables.

---

## File Structure

```
agent-rag-financial-system/
‚îú‚îÄ‚îÄ README.md                    # Project documentation  
‚îú‚îÄ‚îÄ requirements.txt             # Python dependencies (includes LangGraph)
‚îú‚îÄ‚îÄ .gitignore                  # Git ignore rules
‚îú‚îÄ‚îÄ main.py                     # Main CLI interface (enhanced + basic modes)
‚îú‚îÄ‚îÄ src/                       # Organized source code structure
‚îÇ   ‚îú‚îÄ‚îÄ agents/                # LangGraph agents and enhanced RAG
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ query_decomposer.py     # Query decomposition with LangGraph
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ synthesis_engine.py     # Result synthesis and reasoning
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ enhanced_rag.py         # Enhanced RAG pipeline orchestrator
‚îÇ   ‚îú‚îÄ‚îÄ rag/                   # Core RAG components
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ rag_pipeline.py         # Original RAG pipeline (moved)
‚îÇ   ‚îú‚îÄ‚îÄ scrapers/              # Web scraping modules
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ sec_edgar_scraper.py    # SEC scraper (moved)
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ demo_scraper.py         # Demo scraper (moved)
‚îÇ   ‚îî‚îÄ‚îÄ utils/                 # Utility functions
‚îú‚îÄ‚îÄ tests/                     # Test modules
‚îÇ   ‚îú‚îÄ‚îÄ test_scraper.py            # Basic scraper functionality tests
‚îÇ   ‚îî‚îÄ‚îÄ test_rag_system.py         # Comprehensive RAG system tests
‚îú‚îÄ‚îÄ filings/                   # Downloaded 10-K filings (created on scrape)
‚îú‚îÄ‚îÄ demo_filings/              # Demo filings (created by demo_scraper.py)
‚îî‚îÄ‚îÄ vector_db/                 # ChromaDB vector store (created on RAG processing)
```

### Example File Contents
```
filings/
‚îú‚îÄ‚îÄ GOOGL_10K_2022_[accession].htm
‚îú‚îÄ‚îÄ GOOGL_10K_2023_[accession].htm
‚îú‚îÄ‚îÄ GOOGL_10K_2024_[accession].htm
‚îú‚îÄ‚îÄ MSFT_10K_2022_[accession].htm
‚îú‚îÄ‚îÄ MSFT_10K_2023_[accession].htm
‚îú‚îÄ‚îÄ MSFT_10K_2024_[accession].htm
‚îú‚îÄ‚îÄ NVDA_10K_2022_[accession].htm
‚îú‚îÄ‚îÄ NVDA_10K_2023_[accession].htm
‚îî‚îÄ‚îÄ NVDA_10K_2024_[accession].htm
```

## Technical Details

### RAG Pipeline Architecture

#### 1. Text Extraction (`TextExtractor`)
- Parses HTML SEC filings using BeautifulSoup
- Removes scripts, styles, and formatting
- Cleans and normalizes text content

#### 2. Chunking (`TextChunker`) 
- **Token Range**: 50-1000 tokens per chunk
- **Semantic Boundaries**: Splits on paragraphs and sentences
- **Token Counting**: tiktoken (with word-based fallback)
- **Metadata**: Preserves company, year, filename information

#### 3. Embeddings (`EmbeddingService`)
- **Primary**: Azure OpenAI text-embedding-ada-002
- **Fallback**: Deterministic mock embeddings (1536-dim)
- **Offline Support**: Full functionality without internet

#### 4. Vector Storage (`VectorStore`)
- **Database**: ChromaDB with persistent storage
- **Features**: Similarity search, metadata filtering
- **Performance**: Efficient retrieval with distance scoring

#### 5. Query Pipeline (`RAGPipeline`)
- **Processing**: End-to-end document ingestion
- **Search**: Semantic similarity with top-k results
- **Metadata**: Rich context for each result

### Dependencies

#### Core RAG Dependencies
- `chromadb>=0.4.15` - Vector database for embeddings
- `openai>=1.0.0` - Azure OpenAI API integration
- `tiktoken>=0.5.0` - Token counting and text processing

#### Enhanced LangGraph Dependencies (New!)
- `langgraph>=0.6.0` - LangGraph workflow orchestration
- `langchain>=0.3.0` - LangChain core components
- `langchain-openai>=0.3.0` - LangChain OpenAI integration

#### Web Scraping Dependencies
- `requests>=2.31.0` - HTTP requests to SEC API
- `pandas>=2.0.0` - Data manipulation
- `beautifulsoup4>=4.12.0` - HTML parsing for SEC pages
- `lxml>=4.9.0` - XML/HTML parser backend
- `python-dateutil>=2.8.0` - Date parsing utilities

### SEC EDGAR API

The scraper uses the official SEC EDGAR API:
- **Submissions API**: `https://data.sec.gov/submissions/CIK{CIK}.json`
- **Archives**: `https://www.sec.gov/Archives/edgar/data/{CIK}/{accession}/`

### Rate Limiting

- Implements 0.1-second delays between requests (well below SEC's 10 requests/second limit)
- Additional delays between companies (2 seconds) and downloads (1 second)
- Automatic retry logic with exponential backoff

### Error Handling

- Network connectivity issues
- Missing filings for specific years
- Malformed SEC responses
- File system errors

### Demo Mode

For testing without internet access:
```bash
python main.py rag --process --input-dir filings  # Process them
python main.py rag  # Interactive query mode
```

## API Reference

### CLI Commands

```bash
# Main interface
python main.py {scrape,rag} [options]

# Scraper mode
python main.py scrape --companies GOOGL MSFT NVDA --years 2022 2023 2024 --output-dir filings

# RAG mode  
python main.py rag --process --input-dir demo_filings --vector-store ./vector_db
python main.py rag --query "Your question" --top-k 5
```

### Python API

#### RAG Pipeline
```python
from rag_pipeline import RAGPipeline

# Initialize pipeline
rag = RAGPipeline(vector_store_path="./vector_db")

# Process documents
results = rag.process_directory("demo_filings")

# Query system
response = rag.query("What are the risk factors?", top_k=5)
```

#### SEC Scraper
```python
from sec_edgar_scraper import SECEdgarScraper

# Initialize scraper
scraper = SECEdgarScraper(user_agent="Your App 1.0")

# Download filings
files = scraper.scrape_company_10k_filings("GOOGL", [2023, 2024], "output_dir")
```

## Performance Notes

- **Processing Speed**: ~1-2 seconds per document for demo files
- **Storage**: Vector database grows ~50MB per 100 documents
- **Memory Usage**: ~200MB baseline + ~50MB per 1000 chunks
- **Query Speed**: Sub-second response for similarity search

## License

This project is for educational and research purposes. Please ensure compliance with SEC terms of service when using the scraper.
